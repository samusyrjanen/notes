name: Deploy model
description: Deploy the model as an inference service to KServe
inputs:
- {name: model_name, type: String, description: the name of the deployed inference
    service}
- {name: storage_uri, type: String, description: the S3 URI of the saved model in
    MLflow's artifact store (MinIO)}
implementation:
  container:
    image: python:3.10
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kserve==0.10.1' 'kfp==1.8.22' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def deploy_model(model_name: str, storage_uri: str):
          """
          Deploy the model as an inference service to KServe
          Args:
              model_name: the name of the deployed inference service
              storage_uri: the S3 URI of the saved model in MLflow's artifact store (MinIO)
          """
          from kubernetes import client
          from kserve import KServeClient
          from kserve import constants
          from kserve import V1beta1InferenceService
          from kserve import V1beta1InferenceServiceSpec
          from kserve import V1beta1PredictorSpec
          from kserve import V1beta1SKLearnSpec
          import logging

          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          model_uri = storage_uri
          logger.info(f"MODEL URI: {model_uri}")
          namespace = "kserve-inference"
          kserve_version="v1beta1"
          api_version = constants.KSERVE_GROUP + '/' + kserve_version

          isvc = V1beta1InferenceService(
              api_version=api_version,
              kind=constants.KSERVE_KIND,
              metadata=client.V1ObjectMeta(
                  name=model_name,
                  namespace=namespace,
              ),
              spec=V1beta1InferenceServiceSpec(
                  predictor=V1beta1PredictorSpec(
                      service_account_name='kserve-sa',
                      sklearn=V1beta1SKLearnSpec(
                          storage_uri=model_uri
                      )
                  )
              )
          )
          kserve = KServeClient()
          try:
              kserve.create(inferenceservice=isvc)
          except RuntimeError:
              kserve.patch(name=model_name, inferenceservice=isvc, namespace=namespace)

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - deploy_model
