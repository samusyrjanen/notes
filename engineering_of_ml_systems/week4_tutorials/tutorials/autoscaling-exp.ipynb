{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizontal autoscaling experiment\n",
    "This experiment shows how horizontal autoscaling can maintain the performance of an inference service in user traffic spikes. \n",
    "\n",
    "The same sklearn red wine model was used in the experiment. We first disabled the horizontal scaling of the inference service. As a result, only one pod was running for the inference service regardless of user demand. Later we updated the inference service by enabling the horizontal autoscaling:\n",
    "```yaml\n",
    "spec:\n",
    "  predictor:\n",
    "    minReplicas: 1\n",
    "    maxReplicas: 5\n",
    "    scaleTarget: 1000\n",
    "    scaleMetric: concurrency\n",
    "```\n",
    "We simulated 4500 concurrent POST requests for 30 seconds and sent them to the sklearn red wine inference service. The results show that horizontal autoscaling can shorten the response time of an inference service in user traffic spikes:\n",
    "|                       | With autoscaling | Without autoscaling |\n",
    "|-----------------------|------------------|-------------------- |\n",
    "| Fastest response time | 0.0248s          | 0.0225s             |\n",
    "| Slowest response time | 15.3414s         | 45.4707s            |\n",
    "| Average response time | 7.7644s          | 17.3992s            |\n",
    "\n",
    "\n",
    "### Details of experiment setup\n",
    "The YAML files used in this experiment can be found from the \"manifests\" directory located at the same place with this Markdown file. \n",
    "\n",
    "### 1. Deploy an sklearn red wine inference service and disable horizontal autoscaling\n",
    "```bash\n",
    "kubectl apply -f manifests/exp-no-scale.yaml\n",
    "```\n",
    "Load the inference service using hey\n",
    "```bash\n",
    "model_name=redwine-exp\n",
    "input_path=redwine-input.json\n",
    "host=${model_name}.kserve-inference.example.com\n",
    "url=http://kserve-gateway.local:30200/v1/models/${model_name}:predict\n",
    "\n",
    "# Simulate 4500 concurrent POST requests for 30 seconds\n",
    "# \"-t 0\" means infinite timeout for each request\n",
    "hey -t 0 -z 30s -c 4500 -m POST -host ${host} -D ${input_path} ${url}\n",
    "```\n",
    "\n",
    "### 2. Update the inference service by enabling horizontal autoscaling\n",
    "```bash\n",
    "kubectl apply -f manifests/exp-scale.yaml\n",
    "```\n",
    "Then load the updated inference service using the same command as before.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
