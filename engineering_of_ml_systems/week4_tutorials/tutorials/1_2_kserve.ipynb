{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KServe\n",
    "We recall that KServe is responsible for serving models. To deploy a model as an inference service to KServe, we need to specify the following information:\n",
    "- the name of the inference service\n",
    "- the model format (TensorFlow, PyTorch, .etc)\n",
    "- the location of the model artifact\n",
    "\n",
    "After the information above is specified, KServe will perform the following tasks to deploy the model:\n",
    "1. Choosing a model server based on the specified model format and creating a Kubernetes service using the Docker image of the chosen model server. KServe supports multiple types of model servers, such as Triton, TFServing, TorchServe, and SKLearn MLServer. KServe also allows users to build custom model servers using KServe SDK but we will mainly use pre-built model servers in this course.\n",
    "2. Download the model artifact from the specified location.\n",
    "3. Configuring networking resources to expose the inference service to clients.\n",
    "\n",
    "*More reading material: [KServe docs](https://kserve.github.io/website/0.10/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KServe example\n",
    "This example shows how to deploy the red wine model you trained in the first week's MLflow tutorial to KServe. \n",
    "\n",
    "Before going to the examples, we need to obtain the location where the model is saved. Recall that the model was uploaded to MLflow. \n",
    "\n",
    "Just like what you did in the example of model-in-a-service deployment pattern, navigate to the MLflow service [http://mlflow-server.local](http://mlflow-server.local) and then to the \"mlflow-minio-test\" experiment. Click the MLflow run (the \"Start Time\" column) that produced an ElasticNet model for wine quality prediction. Then keep a note of the full S3 URI of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a model to KServe\n",
    "There are two ways of deploying a model to KServe, kubectl and the KServe Python SDK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: Applying a YAML file using kubectl\n",
    "\n",
    "First open [manifests/redwine-model.yaml](./manifests/redwine-model.yaml) and change the `storageUri` to the S3 URI of the model artifact's location you got when running the previous MLflow tutorial, e.g., \n",
    "```yaml\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-sa \n",
    "    model:\n",
    "      modelFormat: \n",
    "        name: sklearn\n",
    "      storageUri: <the-s3-uri-of-your-mode-artifact>\n",
    "```\n",
    "**Note**: Change the `storageUri` in the Yaml file, not in this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a deeper look at the YAML file\n",
    "```yaml\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"sklearn-redwine\"\n",
    "  namespace: kserve-inference\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-sa \n",
    "    model:\n",
    "      modelFormat: \n",
    "        name: sklearn\n",
    "      storageUri: <the-s3-uri-of-your-mode-artifact>\n",
    "```\n",
    "From a high level, this YAML file defines an InferenceService object in KServe. An InferenceService is a Kubernetes resource that represents a deployed model.\n",
    "\n",
    "Now we break the file into several parts:\n",
    "* `apiVersion: \"serving.kserve.io/v1beta1\"`: This field specifies the API version of KServe that is being used to define the InferenceService. It tells Kubernetes how to interpret the content of the YAML file.\n",
    "* `kind: \"InferenceService\"`: This field specifies the object type being deployed to K8s. In this case, we want to deploy an InferenceService object. \n",
    "* `metadata`: This section contains metadata information about the InferenceService\n",
    "  * `name: sklearn-redwine`: This is the user-defined name to identify the object. \n",
    "  * `namespace: kserve-inference`: This is the namespace where the InferenceService object is deployed. Namespaces are used to isolate resources and avoid naming conflicts.\n",
    "* `spec`: This section defines the specification of the InferenceService, which includes the model configuration and serving details.\n",
    "  * `predictor`: This field specifies the model used in inference. \n",
    "    * `serviceAccountName: kserve-sa`: This is the name of the Kubernetes service account that will be used by the InferenceService. Service accounts provide the necessary permissions and access control for an object to interact with other resources in the cluster. In this case, it contains the MinIO credentials (username and password) that will be used by KServe to download model artifacts from the MinIO storage service. The service account was configured during the course environment setup. \n",
    "    * `name: sklearn`: This field tells KServe that the model an Sklearn model so that KServe can prepare the appropriate runtime environment for the model. \n",
    "      * `storageUri: ...`: This is the storage URI of the model artifact. It tells KServe where to download the model artifact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f manifests/redwine-model.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine-week4 created\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the inference service was deployed correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get isvc redwine-week4 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
    "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   115s\n",
    "```\n",
    "**Note**: \n",
    "- The value in the \"AGE\" field may vary depending on how long the inference service has been running. \n",
    "- It may take a few minutes for the inference service to become ready, so please wait for a while and rerun the command in the previous cell if READY is Unknown (i.e. not equal to True). Or you can use the \"-w\" option to continuously watch the status of the inference service (`kubectl get isvc redwine-week4 -n kserve-inference -w`) and then terminate the code cell when the inference service is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check that there is a pod for serving the red-wine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"-l\" to search pods with a specific label. Specifically, here we search the pods running for the \"redwine-week4\" inference service. The label is added automatically be KServe\n",
    "!kubectl get pods -n kserve-inference -l serving.kserve.io/inferenceservice=redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see there is a pod whose name is \"redwine-week4-predictor-...\" running, e.g., \n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-8cff9c9f-6hs8f   2/2     Running   0          8m15s\n",
    "```\n",
    "Similar to the inference service, The AGE of the pod may vary depending on how long the pod has been running. The suffix (8cff9c9f-6hs8f in the example) also varies as it's a random string assigned by K8s. \n",
    "\n",
    "As we can see from the output, the redwine-week4-predictor pod has two containers (Ready is 2/2): 1) an application container that provides the runtime for serving the model (i.e., the container where model inference happens) and 2) a proxy container that forwards user traffic to the application container. This proxy container also collects some statistics such as the number of requests the model receives, which can be used in monitoring. Recall that in the \"spec\" field of manifests/redwine-model.yaml, we specify that the model is an sklearn model.\n",
    "```yaml\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat: \n",
    "        name: sklearn\n",
    "```\n",
    "KServe can then choose the appropriate runtime to serve the model based on the given model type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the inference service by sending requests\n",
    "Now it's time to send prediction requests to your inference service running in KServe.\n",
    "\n",
    "KServe uses an ingress gateway to route the incoming requests to the appropriate inference services. In our setup, the ingress gateway is listening at http://kserve-gateway.local:30200. (It's expected that you see a \"page can't be found\" error if you click the URL.)\n",
    "\n",
    "The runtime that KServe uses to serve our sklearn model follows the v1 inference protocol. An inference protocol can been seen as a set of specifications that define, for example, how a model should be exposed behind an endpoint and the format of the requests. The model's predictions are exposed behind an endpoint following the format: \n",
    "```\n",
    "http://{gateway_host}:{gateway_port}/v1/models/{model_name}:predict\n",
    "```\n",
    "\n",
    "The requests need to follow the format\n",
    "```\n",
    "{\n",
    "  \"instances\": <value>|<(nested)list>|<list-of-objects>\n",
    "}\n",
    "```\n",
    "Besides the v1 inference protocol, another inference protocol, the v2 inference protocol, is also used by some of the KServe's runtimes. You'll see how the v2 inference protocol works in this week's assignment. You don't need the details of these protocols to do the assignment. More details of these protocols can be found from the links below if you're interested. \n",
    "- [v1 inference protocol](https://kserve.github.io/website/0.10/modelserving/data_plane/v1_protocol/)\n",
    "- [v2 inference protocol](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "input_sample = [\n",
    "        [7.8, 0.58, 0.02, 2, 0.073, 9, 18, 0.9968, 3.36, 0.57, 9.5],\n",
    "        [8.9, 0.22, 0.48, 1.8, 0.077, 29, 60, 0.9968, 3.39, 0.53, 9.4]\n",
    "    ]\n",
    "model_name = \"redwine-week4\"\n",
    "req_data = {\n",
    "    \"instances\": input_sample\n",
    "}\n",
    "headers = {}\n",
    "\n",
    "# Define Host in the request so that the ingress gateway knows how to forward the request \n",
    "# to the correct inference service\n",
    "headers[\"Host\"] = f\"{model_name}.kserve-inference.example.com\"\n",
    "url = f\"http://kserve-gateway.local:30200/v1/models/{model_name}:predict\"\n",
    "result = requests.post(url, json=req_data, headers=headers)\n",
    "print(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "{'predictions': [5.657319539336507, 5.529618438168187]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the inference service\n",
    "!kubectl delete isvc redwine-week4 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Using the KServe Python SDK\n",
    "KServe provides a Python client SDK. \n",
    "\n",
    "*More information of the usage of the SDK can be found [here](https://kserve.github.io/website/0.10/sdk_docs/sdk_doc/#documentation-for-client-api).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to replace the `model_uri` to your own model artifact's S3 URI before running the next code cell. \n",
    "\n",
    "The code snippet below can be matched to the YAML file used in Approach 1, as shown in the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1SKLearnSpec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    model_name = \"redwine-week4-2\"\n",
    "    \n",
    "    # replace this with your your own red wine model S3 URI\n",
    "    model_uri = \"s3://mlflow/12/9d75a172ed7543cd9619cb6ab5589258/artifacts/model\"\n",
    "    \n",
    "    namespace = \"kserve-inference\"\n",
    "    kserve_version=\"v1beta1\"\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(\n",
    "        # apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "        # kind: \"InferenceService\"\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "\n",
    "        # metadata:\n",
    "        #   name: \"redwine-week4\"\n",
    "        #   namespace: kserve-inference\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=model_name,\n",
    "            namespace=namespace,\n",
    "        ),\n",
    "\n",
    "        # spec:\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            # predictor\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                # serviceAccountName\n",
    "                service_account_name=\"kserve-sa\",\n",
    "                # model format\n",
    "                sklearn=V1beta1SKLearnSpec(\n",
    "                    # storageUri\n",
    "                    storage_uri=model_uri\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    kserve = KServeClient()\n",
    "\n",
    "    # When applying a YAML file to KServe in Approach 1, KServe will create a new InferenceService if it doesn't yet exist,\n",
    "    # otherwise KServe will patch (modify) the existing InferenceService. \n",
    "    # When using KServe SDK, different methods need to called in the cases of existing and non-existing InferenceService. \n",
    "    try:\n",
    "        kserve.create(inferenceservice=isvc)\n",
    "    except RuntimeError:\n",
    "        # If the inference service with the same name exists\n",
    "        kserve.patch(name=model_name, inferenceservice=isvc, namespace=namespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Start deploying an inference service.')\n",
    "main()\n",
    "logger.info('The inference service has been deployed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the inference service by running the `kubectl get isvc` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get isvc redwine-week4-2 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME              URL                                                   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                       AGE\n",
    "redwine-week4-2   http://redwine-week4-2.kserve-inference.example.com   True           100                              redwine-week4-2-predictor-default-00001   2m1s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our \"redwine-week4-2\" inference service by sending a request to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "input_sample = [\n",
    "        [7.8, 0.58, 0.02, 2, 0.073, 9, 18, 0.9968, 3.36, 0.57, 9.5],\n",
    "        [8.9, 0.22, 0.48, 1.8, 0.077, 29, 60, 0.9968, 3.39, 0.53, 9.4]\n",
    "    ]\n",
    "model_name = \"redwine-week4-2\"\n",
    "req_data = {\n",
    "    \"instances\": input_sample\n",
    "}\n",
    "headers = {}\n",
    "\n",
    "# Define Host in the request so that the ingress gateway knows how to forward the request \n",
    "# to the correct inference service\n",
    "headers[\"Host\"] = f\"{model_name}.kserve-inference.example.com\"\n",
    "url = f\"http://kserve-gateway.local:30200/v1/models/{model_name}:predict\"\n",
    "result = requests.post(url, json=req_data, headers=headers)\n",
    "print(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:  \n",
    "```text\n",
    "{\"predictions\": [5.657319539336507, 5.529618438168187]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the \"redwine-week4-2\" inference service\n",
    "!kubectl delete isvc redwine-week4-2 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"redwine-week4-2\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "You've learned how to deploy a model as an independent inference service. To wrap up the section on model deployment patterns, you can return to [1_deployment_patterns.ipynb](./1_deployment_patterns.ipynb) to explore the pros and cons of both the \"model-in-service\" and \"model-as-service\" deployment patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
