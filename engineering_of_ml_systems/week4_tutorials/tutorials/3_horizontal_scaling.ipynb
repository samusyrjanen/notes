{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Several YAML files will be used to run some examples in this tutorial. Similar to what you did before, you will need to change the \"storareUri\" in these YAML files to the S3 URI of one of your own red wine models saved in MLflow if you want to run the examples yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizontal scaling\n",
    "The capacity of a single server to handle computational tasks is limited. As user traffic surges, it becomes necessary to distribute this traffic among multiple replicas that run your model on different servers. \n",
    "\n",
    "**Horizontal scaling** means adding more servers to ensure that your inference service can respond to increasing user requests.\n",
    "\n",
    "In our K8s setup, horizontal scaling in practice means increasing the pods running for an inference service. In [manifests/redwine-model-scale.yaml](./manifests/redwine-model-scale.yaml), by adding the `minReplicas` field, we specify the minimum number of pods running for the \"redwine-week4\" inference service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/redwine-week4 created\n"
     ]
    }
   ],
   "source": [
    "# Create the \"redwine-week4\" inference service served by 3 pods.\n",
    "!kubectl apply -f manifests/redwine-model-scale.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine-week4 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
      "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   94s\n"
     ]
    }
   ],
   "source": [
    "# Check the \"redwine-week4\" inference service to make sure it's ready\n",
    "!kubectl get isvc redwine-week4 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
    "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   28s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS    RESTARTS   AGE    IP            NODE              NOMINATED NODE   READINESS GATES\n",
      "redwine-week4-predictor-default-00001-deployment-747fbd846l26kx   2/2     Running   0          119s   10.244.2.29   kind-ep-worker    <none>           <none>\n",
      "redwine-week4-predictor-default-00001-deployment-747fbd846l2wf7   2/2     Running   0          118s   10.244.1.26   kind-ep-worker2   <none>           <none>\n",
      "redwine-week4-predictor-default-00001-deployment-747fbd846qh4cl   2/2     Running   0          118s   10.244.2.30   kind-ep-worker    <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "# Check the pods running for the \"redwine-week4\" inference service\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=redwine-week4 -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE     IP             NODE              NOMINATED NODE   READINESS GATES\n",
    "redwine-week4-predictor-default-00002-deployment-5744cfb98j88x2   2/2     Running   0          4m27s   10.244.2.106   kind-ep-worker2   <none>           <none>\n",
    "redwine-week4-predictor-default-00002-deployment-5744cfb98qrzqq   2/2     Running   0          4m27s   10.244.2.107   kind-ep-worker2   <none>           <none>\n",
    "redwine-week4-predictor-default-00002-deployment-5744cfb98z5lvb   2/2     Running   0          4m27s   10.244.1.120   kind-ep-worker    <none>           <none>\n",
    "```\n",
    "The \"NODE\" field shows that these pods are running on different cluster nodes. The IPs in the \"IP\" column can vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "!kubectl -n kserve-inference delete isvc redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal autoscaling\n",
    "User traffic can fluctuate up and down in many use cases. As a result, it is inefficient to manually add more servers when traffic spikes come and remove them when traffic decreases. This is where horizontal autoscaling is needed. By horizontal autoscaling, new servers can be automatically launched when user traffic increases. Similarly, excess servers can be terminated when the traffic decreases. \n",
    "\n",
    "Autoscaling can bring the following benefits:\n",
    "- It allows your inference service to automatically respond changes in demand, minimizing human intervention.\n",
    "- Servers can allocated based on user demand, minimizing the risks of overprovisioning (i.e., launching too many servers than actually needed), thereby saving costs during periods of low demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horizontal autoscaling in KServe\n",
    "KServe offers horizontal autoscaling by default. Let's look at an example in [manifests/redwine-model-autoscale.yaml](./manifests/redwine-model-autoscale.yaml). The content looks familiar, except the following fields:\n",
    "```yaml\n",
    "spec:\n",
    "  predictor:\n",
    "    minReplicas: 1\n",
    "    maxReplicas: 8\n",
    "    scaleTarget: 1\n",
    "    scaleMetric: concurrency\n",
    "```\n",
    "- `minReplicas`: Minimum number of pods running for an inference service.\n",
    "- `maxReplicas`: Maximum number of pods for autoscaling.\n",
    "- `scaleMetric`: The scaling metric. It's used to decided when to scale pods. In this example, the metric is concurrency, which refers to the number of in-flight requests per pod at any given time.\n",
    "- `scaleTarget`: This an integer that specifies the target value of the \"scaleMetric\" that autoscaling should try to satisfy. Note that this target may not always be achieved as the maximum number of pods and/or the computational resources in the cluster is limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/redwine-week4 created\n"
     ]
    }
   ],
   "source": [
    "# Create the \"redwine-week4\" inference service with horizontal autoscaling enabled\n",
    "!kubectl apply -f manifests/redwine-model-autoscale.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine-week4 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
      "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   27s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the \"redwine-week4\" inference service is ready\n",
    "!kubectl get isvc redwine-week4 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
    "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
      "redwine-week4-predictor-default-00001-deployment-7d767d9dcr9mvd   2/2     Running   0          36s\n"
     ]
    }
   ],
   "source": [
    "# Check the pods running for the \"redwine-week4\" inference service\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b9sdcf   2/2     Running   0          68s\n",
    "```\n",
    "Only one pod is running for the \"redwine-week4\" inference service now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's send some requests to the service using a tool named \"hey\". Hey is a small program that send traffic in a controllable way. \n",
    "\n",
    "We will use the following command to simulate user requests:\n",
    "```bash\n",
    "hey -z 10s -c 5 -m POST -host ${host} -D ${input_path} ${url}\n",
    "```\n",
    "Running this command will simulate 5 concurrent POST requests for 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Total:\t10.1390 secs\n",
      "  Slowest:\t0.2430 secs\n",
      "  Fastest:\t0.0207 secs\n",
      "  Average:\t0.0857 secs\n",
      "  Requests/sec:\t58.0925\n",
      "  \n",
      "  Total data:\t31217 bytes\n",
      "  Size/request:\t53 bytes\n",
      "\n",
      "Response time histogram:\n",
      "  0.021 [1]\t|\n",
      "  0.043 [63]\t|■■■■■■■■■■■\n",
      "  0.065 [60]\t|■■■■■■■■■■■\n",
      "  0.087 [222]\t|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "  0.110 [156]\t|■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "  0.132 [44]\t|■■■■■■■■\n",
      "  0.154 [18]\t|■■■\n",
      "  0.176 [9]\t|■■\n",
      "  0.199 [3]\t|■\n",
      "  0.221 [5]\t|■\n",
      "  0.243 [8]\t|■\n",
      "\n",
      "\n",
      "Latency distribution:\n",
      "  10% in 0.0423 secs\n",
      "  25% in 0.0700 secs\n",
      "  50% in 0.0834 secs\n",
      "  75% in 0.0986 secs\n",
      "  90% in 0.1230 secs\n",
      "  95% in 0.1461 secs\n",
      "  99% in 0.2278 secs\n",
      "\n",
      "Details (average, fastest, slowest):\n",
      "  DNS+dialup:\t0.0001 secs, 0.0207 secs, 0.2430 secs\n",
      "  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0028 secs\n",
      "  req write:\t0.0000 secs, 0.0000 secs, 0.0011 secs\n",
      "  resp wait:\t0.0853 secs, 0.0206 secs, 0.2429 secs\n",
      "  resp read:\t0.0001 secs, 0.0000 secs, 0.0055 secs\n",
      "\n",
      "Status code distribution:\n",
      "  [200]\t589 responses\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "model_name=redwine-week4\n",
    "input_path=redwine-input.json\n",
    "host=${model_name}.kserve-inference.example.com\n",
    "url=http://kserve-gateway.local:30200/v1/models/${model_name}:predict\n",
    "\n",
    "# Send 10 seconds of post requests maintaining 5 in-flight requests. The sent data are saved in redwine-input.json\n",
    "hey -z 10s -c 5 -m POST -host ${host} -D ${input_path} ${url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS        RESTARTS   AGE\n",
      "redwine-week4-predictor-default-00001-deployment-7d767d9dc6t7kx   2/2     Terminating   0          98s\n",
      "redwine-week4-predictor-default-00001-deployment-7d767d9dchs2fr   2/2     Terminating   0          101s\n",
      "redwine-week4-predictor-default-00001-deployment-7d767d9dcr9mvd   2/2     Terminating   0          3m42s\n",
      "redwine-week4-predictor-default-00001-deployment-7d767d9dcsn8lr   2/2     Terminating   0          100s\n",
      "redwine-week4-predictor-default-00001-deployment-7d767d9dcvkc6l   2/2     Running       0          102s\n"
     ]
    }
   ],
   "source": [
    "# Check the pods running for the \"redwine-week4\" inference service again\n",
    "# You need to run this command immediately after the hey command is completed\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b5gzxw   2/2     Running   0          24s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b9sdcf   2/2     Running   0          70s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85bhtvfx   2/2     Running   0          22s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85brm5xb   2/2     Running   0          22s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85bxpnp5   2/2     Running   0          20s\n",
    "```\n",
    "Recall that we set `scaleTarget` to 1 in manifests/redwine-model-autoscale.yaml, which means KServe should try to scale up the pods in such a way that each pod will handle one request simultaneously. Since 5 concurrent requests were sent, five pods were needed in total. As a result, KServe launched four more pods to handle the requests. \n",
    "\n",
    "Notice that the number of the additional pods KServe creates can vary depending on how the traffic arrived at the inference service. It's also OK if you see three or five additional pods created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wait for a few minutes and check the pods again (by running the previous code cell), you will see four of pods are being terminated. In other words, the inference service is scaled down as there is no traffic anymore.\n",
    "```text\n",
    "NAME                                                              READY   STATUS        RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b5gzxw   2/2     Terminating   0          76s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b9sdcf   2/2     Running       0          2m2s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85bhtvfx   2/2     Terminating   0          74s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85brm5xb   2/2     Terminating   0          74s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85bxpnp5   2/2     Terminating   0          72s\n",
    "```\n",
    "Finally, only one pod remains:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b9sdcf   2/2     Running       0     4m27s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "!kubectl -n kserve-inference delete isvc redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Credits: The examples are based on [this KServe doc](https://kserve.github.io/website/0.10/modelserving/autoscaling/autoscaling/).*\n",
    "\n",
    "*If you are interested at seeing how horizontal autoscaling can maintain the performance of an inference service, you can take a look at [this experiment](./autoscaling-exp.ipynb). (You don't need to run the experiment yourself. The experiment is used to illustrate how horizontal autoscaling can maintain the performance of an inference service in user traffic spikes.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "You've learned how to horizontally scale an inference service using KServe. Now you can go to [the next tutorial](./4_inference_graph.ipynb) and learn about inference graph. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
