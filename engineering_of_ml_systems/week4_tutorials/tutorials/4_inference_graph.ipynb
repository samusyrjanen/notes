{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference graph\n",
    "Simply deploying one model may not be enough in some complex use cases. For example, you may want to chain multiple models into a pipeline, where one model uses the inferences of another model as inputs. You may want to make the final decisions based on the outputs of multiple models in some use cases. Sometimes you may also want to direct user requests to different models based on user information. KServe offers a feature called \"inference graph\" to allow users to easily benefit from multiple models. \n",
    "\n",
    "We highly recommend reading the concept of inference graph from [this KServe doc](https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md) before going to the example of inference graph below. \n",
    "\n",
    "**Clarification of the Kserve doc of inference graph**: \n",
    "In sections 2.4 (Ensemble Node) and 2.5 (Splitter Node) the \"routes\" in the YAML examples should be \"steps\". For example, instead of\n",
    "```yaml\n",
    "...\n",
    "root:\n",
    "  routerType: Ensemble\n",
    "  routes:\n",
    "  - serviceName: sklearn-iris\n",
    "    name: sklearn-iris\n",
    "  - serviceName: xgboost-iris\n",
    "    name: xgboost-iris\n",
    "...\n",
    "``` \n",
    "the example should be\n",
    "```yaml\n",
    "...\n",
    "root:\n",
    "  routerType: Ensemble\n",
    "  steps:\n",
    "  - serviceName: sklearn-iris\n",
    "    name: sklearn-iris\n",
    "  - serviceName: xgboost-iris\n",
    "    name: xgboost-iris\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference graph example\n",
    "This example shows how to use graph inference to split users into two inference services based on user information. \n",
    "\n",
    "In the beginning, let's deploy two inference services for predicting red wine quality. You need to change the model S3 URIs to your own in [manifests/redwine-model-ig.yaml](./manifests/redwine-model-ig.yaml). Replace the \"storageUri\" with your own ones as shown below. \n",
    "\n",
    "```yaml\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "...\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-sa \n",
    "    model:\n",
    "      modelFormat: \n",
    "        name: sklearn\n",
    "      storageUri: s3://... # change to the S3 URI of your model that was trained using the hyperparameters alpha=0.5 and l1_ratio=0.5. This can be the one you trained when following the first week's MLflow tutorial\n",
    "\n",
    "---\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-sa \n",
    "    model:\n",
    "      modelFormat: \n",
    "        name: sklearn\n",
    "      storageUri: s3://... # change to the S3 URI of your model that was trained using the hyperparameters alpha=0.7 and l1_ratio=0.7. This can be the one you trained when following this week's tutorial of canary deployment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/redwine1 created\n",
      "inferenceservice.serving.kserve.io/redwine2 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy two inference services for predicting red wine quality\n",
    "!kubectl apply -f manifests/redwine-model-ig.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine1 created\n",
    "inferenceservice.serving.kserve.io/redwine2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       URL                                            READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                AGE\n",
      "redwine1   http://redwine1.kserve-inference.example.com   True           100                              redwine1-predictor-default-00001   26s\n"
     ]
    }
   ],
   "source": [
    "# Ensure the \"redwine1\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc redwine1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME       URL                                            READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                AGE\n",
    "redwine1   http://redwine1.kserve-inference.example.com   True           100                              redwine1-predictor-default-00001   107s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                          READY   STATUS    RESTARTS   AGE\n",
      "redwine1-predictor-default-00001-deployment-7ff6449d4-lj4bt   2/2     Running   0          39s\n"
     ]
    }
   ],
   "source": [
    "# Ensure there is one pod running for the \"redwine1\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=redwine1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "redwine1-predictor-default-00001-deployment-7d868c5755-gsdc9   2/2     Running   0          3m20s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       URL                                            READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                AGE\n",
      "redwine2   http://redwine2.kserve-inference.example.com   True           100                              redwine2-predictor-default-00001   57s\n"
     ]
    }
   ],
   "source": [
    "# Similarly, make sure the \"redwine2\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc redwine2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME       URL                                            READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                AGE\n",
    "redwine2   http://redwine2.kserve-inference.example.com   True           100                              redwine2-predictor-default-00001   3m54s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
      "redwine2-predictor-default-00001-deployment-65f6765967-nf56v   2/2     Running   0          59s\n"
     ]
    }
   ],
   "source": [
    "# Also ensure there is one pod running for the \"redwine2\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=redwine2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "redwine2-predictor-default-00001-deployment-57768c7955-bqnb8   2/2     Running   0          4m28s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's deploy an inference graph for the \"redwine1\" and \"redwine2\" inference services. But before the deployment, let's take a look at [manifests/inference-graph.yaml](./manifests/inference-graph.yaml) that specifies the inference graph:\n",
    "```yaml\n",
    "spec: \n",
    "  nodes: \n",
    "    root: \n",
    "      routerType: Switch\n",
    "      steps: \n",
    "      - serviceName: redwine1\n",
    "        condition: \"[@this].#(userId==1)\"\n",
    "      - serviceName: redwine2\n",
    "        condition: \"[@this].#(userId==2)\"\n",
    "```\n",
    "This inference graph includes only one routing node whose type is Switch. This routing node will direct user requests to one of the inference services listed in the `steps` section based on the condition. Suppose the requests are in the form of\n",
    "```python\n",
    "{\n",
    "  \"userId\": 1,\n",
    "  \"instances\": [...] #some wine-related chemical attributes\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the user requests with userId equal to 1 will be routed to the \"redwine1\" inference service. Similarly, requests with userId equal to 2 will be routed to the \"redwine2\" inference service. If the requests have no userId or userId is something else than 1 or 2, the routing node will return the request directly. The condition is defined using the [gjson syntax](https://github.com/tidwall/gjson/blob/master/SYNTAX.md). Let's use the expression `[@this].#(userId==1)` as an example. `[@this]` refers to the request, `.#(userId==1)` checks whether the value of the \"userId\" field of the request is equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencegraph.serving.kserve.io/mygraph created\n"
     ]
    }
   ],
   "source": [
    "# Deploy inference graph\n",
    "!kubectl apply -f manifests/inference-graph.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/mygraph created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      URL                                           READY   AGE\n",
      "mygraph   http://mygraph.kserve-inference.example.com   True    18s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the inference graph named \"mygraph\" is ready\n",
    "!kubectl -n kserve-inference get ig mygraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME      URL                                           READY   AGE\n",
    "mygraph   http://mygraph.kserve-inference.example.com   True    15s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "mygraph-00001-deployment-6499d8b484-jxfks   2/2     Running   0          34s\n"
     ]
    }
   ],
   "source": [
    "# There should be one pod running for the \"mrgraph\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferencegraph=mygraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                        READY   STATUS    RESTARTS   AGE\n",
    "mygraph-00001-deployment-549797bdf8-rh4gc   2/2     Running   0          18m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then send a request to the inference graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Prepare data, headers, and url\n",
    "input_sample = [\n",
    "        [7.8, 0.58, 0.02, 2, 0.073, 9, 18, 0.9968, 3.36, 0.57, 9.5],\n",
    "        [8.9, 0.22, 0.48, 1.8, 0.077, 29, 60, 0.9968, 3.39, 0.53, 9.9]\n",
    "    ]\n",
    "ig_name = \"mygraph\"\n",
    "headers = {}\n",
    "headers[\"Host\"] = f\"{ig_name}.kserve-inference.example.com\"\n",
    "url = \"http://kserve-gateway.local:30200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [5.657319539336507, 5.569620109646147]}\n"
     ]
    }
   ],
   "source": [
    "req_data = {\n",
    "    \"userId\": 1,\n",
    "    \"instances\": input_sample\n",
    "}\n",
    "\n",
    "result = requests.post(url, json=req_data, headers=headers)\n",
    "print(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: `{'predictions': [5.657319539336507, 5.569620109646147]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [5.657319539336507, 5.569620109646147]}\n"
     ]
    }
   ],
   "source": [
    "req_data = {\n",
    "    \"userId\": 2,\n",
    "    \"instances\": input_sample\n",
    "}\n",
    "\n",
    "result = requests.post(url, json=req_data, headers=headers)\n",
    "print(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: {'predictions': [5.74274844741817, 5.566989419987943]}\n",
    "\n",
    "The output is different from the previous one though the input data is the same. This is because the previous two requests have different userIds, so they were directed to different inference services. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userId': 3, 'instances': [[7.8, 0.58, 0.02, 2, 0.073, 9, 18, 0.9968, 3.36, 0.57, 9.5], [8.9, 0.22, 0.48, 1.8, 0.077, 29, 60, 0.9968, 3.39, 0.53, 9.9]]}\n"
     ]
    }
   ],
   "source": [
    "req_data = {\n",
    "    \"userId\": 3,\n",
    "    \"instances\": input_sample\n",
    "}\n",
    "\n",
    "result = requests.post(url, json=req_data, headers=headers)\n",
    "print(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```text\n",
    "{'userId': 3, 'instances': [[7.8, 0.58, 0.02, 2, 0.073, 9, 18, 0.9968, 3.36, 0.57, 9.5], [8.9, 0.22, 0.48, 1.8, 0.077, 29, 60, 0.9968, 3.39, 0.53, 9.9]]}\n",
    "```\n",
    "\n",
    "The userId of this request is not 1 or 2. In other words, this request does not match any conditions specified for the inference graph. As a result, this request wasn't further directed to any inference services and was directly returned by the routing node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencegraph.serving.kserve.io \"mygraph\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the inference graph\n",
    "!kubectl -n kserve-inference delete -f manifests/inference-graph.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferencegraph.serving.kserve.io \"mygraph\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"redwine1\" deleted\n",
      "inferenceservice.serving.kserve.io \"redwine2\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the \"redwine1\" and \"redwine2\" inference services\n",
    "!kubectl -n kserve-inference delete -f manifests/redwine-model-ig.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"redwine1\" deleted\n",
    "inferenceservice.serving.kserve.io \"redwine2\" deleted\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
