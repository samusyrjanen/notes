{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment strategy: Canary deployment\n",
    "Let's consider a scenario where you have previously deployed a model to production. Now, you have developed a new model and want to use it to replace the old version. A straightforward approach is directly deleting the old model and deploying the new one, which means all users will switch to the new model at the same time. However, this approach is risky. If the new one doesn't perform as well as expected, all users will be unsatisfied. \n",
    "\n",
    "**Canary deployment** is one approach to minimize this risk and ensure a smooth transition. In canary deployment, user requests are gradually shifted to the new model. In other words, cwe first experiments the new model with a small portion of real users and can then direct more users to the new model if the new model performs better than the old one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of canary deployment\n",
    "KServe provides a convenient way to employ canary deployment. The following example shows how to use canary deployment in KServe. \n",
    "\n",
    "*Credits: This example is adapted from [this KServe doc](https://kserve.github.io/website/0.10/modelserving/v1beta1/rollout/canary-example/).*\n",
    "\n",
    "#### Deploy the first version of a model\n",
    "We first deploy a redwine model to KServe.\n",
    "\n",
    "Remember to replace the \"storageUri\" in [manifests/redwine-model.yaml](./manifests/redwine-model.yaml) with your own sklearn redwine model's S3 URI (e.g., the one you trained when following the first week's MLflow tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/redwine-week4 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the first version\n",
    "!kubectl apply -f manifests/redwine-model.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine-week4 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
      "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   56s\n"
     ]
    }
   ],
   "source": [
    "# Check if the \"redwine-week4\" inference service is ready.\n",
    "!kubectl get isvc redwine-week4 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
    "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   17s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a new model\n",
    "Now let's train a new red wine model with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using MLflow tracking URI: http://mlflow-server.local\n",
      "INFO:__main__:Using MLflow experiment: mlflow-minio-test\n",
      "INFO:__main__:Fitting model...\n",
      "INFO:__main__:Finished fitting\n",
      "INFO:__main__:Elasticnet model (alpha=0.700000, l1_ratio=0.700000):\n",
      "INFO:__main__:  RMSE: 0.832725785743381\n",
      "INFO:__main__:Logging parameters to MLflow\n",
      "INFO:__main__:Logging trained model\n",
      "/home/user/anaconda3/envs/mlops_eng/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "Registered model 'ElasticnetWineModel' already exists. Creating a new version of this model...\n",
      "2023/11/27 12:31:40 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: ElasticnetWineModel, version 2\n",
      "Created version '2' of model 'ElasticnetWineModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S3 URI of the logged model: s3://mlflow/4/4a3b6ddfa01240d99f7fd564dbc997a3/artifacts/model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Set an environmental variable named \"MLFLOW_S3_ENDPOINT_URL\" so that MLflow client knows where to save artifacts.\n",
    "# MLFLOW_S3_ENDPOINT_URL is the URL of the MinIO storage service\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "\n",
    "# Configure the credentials needed for accessing the MinIO storage service\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"http://mlflow-server.local\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"mlflow-minio-test\"\n",
    "\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def main():\n",
    "    np.random.seed(40)\n",
    "\n",
    "    # Read the wine-quality csv file from the URL\n",
    "    csv_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    \n",
    "\n",
    "    data = pd.read_csv(csv_url, sep=\";\")\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train, test = train_test_split(data)\n",
    "\n",
    "    # The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "    train_x = train.drop([\"quality\"], axis=1)\n",
    "    test_x = test.drop([\"quality\"], axis=1)\n",
    "    train_y = train[[\"quality\"]]\n",
    "    test_y = test[[\"quality\"]]\n",
    "    \n",
    "    # Change parameters, both of them were 0.5 before\n",
    "    alpha = 0.7\n",
    "    l1_ratio = 0.7\n",
    "\n",
    "    logger.info(f\"Using MLflow tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "    logger.info(f\"Using MLflow experiment: {MLFLOW_EXPERIMENT_NAME}\")\n",
    "    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "\n",
    "        logger.info(\"Fitting model...\")\n",
    "\n",
    "        lr.fit(train_x, train_y)\n",
    "\n",
    "        logger.info(\"Finished fitting\")\n",
    "\n",
    "        predicted_qualities = lr.predict(test_x)\n",
    "\n",
    "        rmse = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "        logger.info(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" %\n",
    "                    (alpha, l1_ratio))\n",
    "        logger.info(\"  RMSE: %s\" % rmse)\n",
    "\n",
    "\n",
    "        logger.info(\"Logging parameters to MLflow\")\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        logger.info(\"Logging trained model\")\n",
    "        artifact_name = \"model\"\n",
    "        logged_model_info = mlflow.sklearn.log_model(\n",
    "            lr, artifact_name, registered_model_name=\"ElasticnetWineModel\")\n",
    "        print(\"The S3 URI of the logged model:\", mlflow.get_artifact_uri(artifact_path=artifact_name))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the new version of the redwine model. Remember to also replace the \"storageUri\" field in [manifests/redwine-model-v2.yaml](./manifests/redwine-model-v2.yaml) with the S3 URI of the new model (the URI should be printed in the output after running the previous cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/redwine-week4 configured\n"
     ]
    }
   ],
   "source": [
    "# Update the \"redwine-week4\" inference service using the newer model version.\n",
    "# Notice that redwine-model.yaml and redwine-model-v2.yaml have the same namespace and name in the metadata field, \n",
    "# so K8s knows it should update the inference service instead of creating a new one.\n",
    "!kubectl apply -f manifests/redwine-model-v2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine-week4 configured\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION                   LATESTREADYREVISION                     AGE\n",
      "redwine-week4   http://redwine-week4.kserve-inference.example.com   True    90     10       redwine-week4-predictor-default-00001   redwine-week4-predictor-default-00002   6m36s\n"
     ]
    }
   ],
   "source": [
    "# Check that the updated \"redwine-week4\" inference service is ready\n",
    "!kubectl get isvc redwine-week4 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```text\n",
    "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION                   LATESTREADYREVISION                     AGE\n",
    "redwine-week4   http://redwine-week4.kserve-inference.example.com   True    90     10       redwine-week4-predictor-default-00001   redwine-week4-predictor-default-00002   3m19s\n",
    "```\n",
    "From the output, you can see the incoming traffic is split between the new model (10%) and the previous model (90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at what happened. \n",
    "\n",
    "The content of redwine-model-v2.yaml is almost the same as redwine-model.yaml. Only the `storageURI` is updated and a new field `canaryTrafficPercent` is added to redwine-model-v2.yaml. `canaryTrafficPercent` indicates the percentage of user traffic that need to be directed to the new model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the pods running for the \"redwine-week4\" inference service,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
      "redwine-week4-predictor-default-00001-deployment-7c65c589fgsmcv   2/2     Running   0          7m59s\n",
      "redwine-week4-predictor-default-00002-deployment-56f78f67cm6flc   2/2     Running   0          3m42s\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-65f7fddfb6n9pj   2/2     Running   0          2m17s\n",
    "redwine-week4-predictor-default-00002-deployment-77dbdbd7b8xndb   2/2     Running   0          94s\n",
    "```\n",
    "You can see there are two pods, the one that contains \"default-00001\" in its name is serving the old model and another the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the new model performs well, we can direct more traffic to it by updating the `canaryTrafficPercent` field in redwine-model-v2.yaml. Finally we can direct all traffic to the model by removing the `canaryTrafficPercent` field (see [manifests/redwine-model-v2-fully-rollout.yaml](./manifests/redwine-model-v2-fully-rollout.yaml)). \n",
    "\n",
    "Remember to also replace the `storageUri` with your own new red wine model's S3 URI in redwine-model-v2-fully-rollout.yaml. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/redwine-week4 configured\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f manifests/redwine-model-v2-fully-rollout.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine-week4 configured\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the \"redwine-week4\" inference service again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
      "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00002   10m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!kubectl get isvc redwine-week4 -n kserve-inference -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
    "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00002   17h\n",
    "```\n",
    "Now 100% traffic is directed to the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the pods for running the \"redwine-week4\" inference service again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
      "redwine-week4-predictor-default-00002-deployment-56f78f67cm6flc   2/2     Running   0          7m19s\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00002-deployment-77dbdbd7b8xndb   2/2     Running   0          3m8s\n",
    "```\n",
    "You can notice the pod (with \"default-00001\" in its name) serving the old model is terminated and only the pod (with \"default-00002\" in its name) remains and continue serving the new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up by deleting the \"redwine-week4\" inference service\n",
    "!kubectl delete isvc redwine-week4 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "You've learned how to apply canary deployment when release a model to production. You can now go to [the next tutorial](./3_horizontal_scaling.ipynb) and see how to scale a model in response to increased traffic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
